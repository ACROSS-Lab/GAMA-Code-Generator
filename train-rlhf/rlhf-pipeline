1 - Pretraining a language model (LM)
STF - data = train data 151
Pretrained = Mistral7B
STF model = GAML Mistral 7B 151 500

2 - Training a reward model
- collecting a dataset of (input text, output text, reward) triplets.
With this new dataset, we will train another language model to receive the (input, output) text and return a reward scalar! This will be our reward model.

The main objective here is to use the reward model to mimic the human's reward labeling and therefore be able to do RLHF training offline, without the human in the loop.
