{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e96db59a99b64fba83a416a42ffc34af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5414c837886c4374a8dc6edd8fd50554",
              "IPY_MODEL_0b88e31c55f24feca59303cab68a95aa",
              "IPY_MODEL_760c682ac04e4dadb58579f73765ee80"
            ],
            "layout": "IPY_MODEL_f300ef0513134f8f8a6ce21e38bd3de7"
          }
        },
        "5414c837886c4374a8dc6edd8fd50554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d4ce8b45664f95bba494ccb0c7aaae",
            "placeholder": "​",
            "style": "IPY_MODEL_0cdf4f7c29744eef95f1762cb4965549",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0b88e31c55f24feca59303cab68a95aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28ab8dca5b4b4ac5b638897bde93f801",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f472e18f02fc4b51bea225246c696470",
            "value": 2
          }
        },
        "760c682ac04e4dadb58579f73765ee80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c43aaf9cb25648fc9ce97ef371c60308",
            "placeholder": "​",
            "style": "IPY_MODEL_41ea07f7290b4c808e2affbcce1f21d6",
            "value": " 2/2 [00:14&lt;00:00,  6.89s/it]"
          }
        },
        "f300ef0513134f8f8a6ce21e38bd3de7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7d4ce8b45664f95bba494ccb0c7aaae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cdf4f7c29744eef95f1762cb4965549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28ab8dca5b4b4ac5b638897bde93f801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f472e18f02fc4b51bea225246c696470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c43aaf9cb25648fc9ce97ef371c60308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41ea07f7290b4c808e2affbcce1f21d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "yLAxT9er9_6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8-d2usENQOF",
        "outputId": "0d799b34-e02c-4cd3-b7f7-35db9b0796f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install -i https://test.pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CxFzGi7nRix",
        "outputId": "dcfcf792-4d99-413b-9e72-07ddf6991bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-2_1deolz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-2_1deolz\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 0b192de1f353b0e04dad4813e02e2c672de077be\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.39.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium chromium-driver\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhWftLivnWoi",
        "outputId": "1a497de3-2030-42b4-f2d5-cd85a0785a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.l5UOnplrp9/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
            "gpg: key DCC9EFBF77E11517: \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.VVAwmmYDWq/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
            "gpg: key DC30D7C23CBBABEE: \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.bURCZOHq1l/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
            "gpg: key 4DFAB270CAA96DFA: \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "gpg: [stdout]: write error: Broken pipe\n",
            "gpg: filter_flush failed on close: Broken pipe\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "gpg: [stdout]: write error: Broken pipe\n",
            "gpg: filter_flush failed on close: Broken pipe\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "gpg: [stdout]: write error: Broken pipe\n",
            "gpg: filter_flush failed on close: Broken pipe\n",
            "Hit:1 http://deb.debian.org/debian buster InRelease\n",
            "Hit:2 http://deb.debian.org/debian buster-updates InRelease\n",
            "Hit:3 http://deb.debian.org/debian-security buster/updates InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium is already the newest version (90.0.4430.212-1~deb10u1).\n",
            "chromium-driver is already the newest version (90.0.4430.212-1~deb10u1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHo8GIEonhDF",
        "outputId": "f320bf8d-176b-4744-90b5-c4de083e0147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.13.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.4)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.22.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install chromium-chromedriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk-gJ6S8nbTb",
        "outputId": "35923bb4-7782-49ee-9b72-1c55896c6488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  chromium-chromedriver\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 2,308 B of archives.\n",
            "After this operation, 77.8 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Fetched 2,308 B in 0s (5,114 B/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 121701 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "dpkg: error processing archive /var/cache/apt/archives/chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb (--unpack):\n",
            " trying to overwrite '/usr/bin/chromedriver', which is also in package chromium-driver 90.0.4430.212-1~deb10u1\n",
            "Errors were encountered while processing:\n",
            " /var/cache/apt/archives/chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb\n",
            "E: Sub-process /usr/bin/dpkg returned an error code (1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzuvzgr4nfP0",
        "outputId": "81f05262-deb9-4025-a1fb-6420796d21b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
        "import torch\n",
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "WUq-GFT6ndw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoDataset(Dataset):\n",
        "    def __init__(self, text_dataset, tokenizer, max_length=1024):\n",
        "        self.max_length = max_length\n",
        "        self.text_dataset = text_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for text in text_dataset:\n",
        "          print(\"text\", text)\n",
        "          encodings_dict = self.tokenizer(text, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
        "\n",
        "          self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "          self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n"
      ],
      "metadata": {
        "id": "A-qfRBUwviFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReasoningLLM:\n",
        "  def __init__(self, model_name=None, prompt_blueprint=\"\"\"\n",
        "  [INST]\n",
        "  <<SYS>>\n",
        "  %s\n",
        "  <</SYS>>\n",
        "  %s\n",
        "  [/INST]\n",
        "  \"\"\"):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    self.prompt_blueprint = prompt_blueprint\n",
        "    # - Utilize the hint below to assist you with your Google Search Query\n",
        "\n",
        "    # HINT:\n",
        "    # An example of REAL WORLD DATA is if the LLM prompt is related to extracting information from a job page you would respond with a Google Search Query for job page websites\n",
        "\n",
        "  def randomize_query(self, search_query):\n",
        "    system_prompt = \"\"\"\n",
        "    You are a CREATIVE Search Query Randomizer Bot, your role is to get a search query given to you by the user and randomize the information in the search query.\n",
        "    You must follow the instrutions below:\n",
        "    - Analyze the given google search query and think through what topics/specifics make up the given search query\n",
        "    - Then, I want you to then think of a list of possible google search queries that would come up with similar results, but for a different, random topic.\n",
        "    - Rewrite the search query with the random/different topics you thought of\n",
        "    - The new sentence should be about something different while keeping the same overall purpose\n",
        "    - Make sure the new search query you write is a logical and complete sentence.\n",
        "\n",
        "    The user will give the search query in the format below\n",
        "    SEARCH QUERY: the search query will go here\n",
        "\n",
        "    Finally, when you respond with your new search query use the prefix tag \"NEW-SEARCH-QUERY:\" BEFORE you generate the rewritten search query.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    SEARCH QUERY: {search_query}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = self.prompt_blueprint % (system_prompt, user_prompt)\n",
        "\n",
        "    sequences = self.pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=40,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=self.tokenizer.eos_token_id,\n",
        "        max_length=10000,\n",
        "    )\n",
        "\n",
        "    result = sequences[0]['generated_text']\n",
        "    prefix_tag = \"NEW-SEARCH-QUERY:\"\n",
        "    prefix_index = result.rfind(prefix_tag)\n",
        "    try:\n",
        "      return result[prefix_index+len(prefix_tag):].split('\\n')[0]\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "  def get_google_query(self, input_examples):\n",
        "    system_prompt = \"\"\"\n",
        "    You are a LLM researcher bot, given a example LLM (large language model) prompt I want you to response with a Google Search Query given the instructions below.\n",
        "    You must follow the instructions below:\n",
        "    - Analyze the example LLM prompt you are given and determine what type of information would be needed to train an LLM to complete the general prompt task.\n",
        "    - Using your knowledge of machine learning and large language models, determine what type of website data would be useful for training an LLM to accurately respond to the prompts similar but not the same as the given prompt. You must think of a google search query that will get different information that the specific information in the prompt.\n",
        "    - Now imagine you had to google search for this general LLM prompt data. What sort of Google search would you make?\n",
        "    - If there is a specific website you think would have this REAL WORLD DATA add the 'site:' google filter to the query with the site that would be ideal\n",
        "    - REMEMBER: Only generate a Google Search Query that will find real world data you would find on a website (Example: if the LLM prompt is for extracting info from a job page, respond with a Google Search Query for indeed or monster jobs)\n",
        "    - Respond with an example Google Search Query that would give you the data needed to train the LLM you thought of.\n",
        "\n",
        "    The user will give the example LLM prompt in the format below\n",
        "    EXAMPLE PROMPT: the prompt will go here\n",
        "\n",
        "    Before you generate your Google Search Query, you MUST respond with the prefix tag \"GOOGLE-SEARCH-QUERY:\" BEFORE the query you generate.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    EXAMPLE PROMPT: {input_examples[0]}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = self.prompt_blueprint % (system_prompt, user_prompt)\n",
        "\n",
        "    sequences = self.pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=40,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=self.tokenizer.eos_token_id,\n",
        "        max_length=10000,\n",
        "    )\n",
        "\n",
        "    result = sequences[0]['generated_text']\n",
        "    prefix_tag = \"GOOGLE-SEARCH-QUERY:\"\n",
        "    prefix_index = result.rfind(prefix_tag)\n",
        "    try:\n",
        "      return result[prefix_index+len(prefix_tag):].split('\\n')[0]\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "  def clean_raw_website_data(self, raw_data, example):\n",
        "    system_prompt = \"\"\"\n",
        "    You are are a data cleaner bot, your role is to take in raw website data and parse out all the relevant information.\n",
        "    You must follow the instructions below\n",
        "    - You must extract all information that's formatted similar to the example provided\n",
        "    - You must analyze the given example information and determine what type of data it is (for example: an article, a phrase, a poem, it could be anything)\n",
        "    - Ignore the specific information in the example info you ONLY care about what type of information it is and how to extract that info type from the raw website data.\n",
        "    - Extract all information from the raw data that matches the type of information you determined from the example\n",
        "    - remove all headers, footers, and general website information that is not considered content.\n",
        "\n",
        "    The raw website data will be provided in the format below:\n",
        "    RAW WEBSITE DATA: the raw website data will go here\n",
        "\n",
        "    The example will be provided in the format below:\n",
        "    EXAMPLE-INFO: the example will go here\n",
        "\n",
        "    You must only respond with the cleaned website data. Do not respond with any explaination or affirmative. Only respond with the cleaned data.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    EXAMPLE-INFO: {example}\n",
        "\n",
        "    RAW WEBSITE DATA: {raw_data}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = self.prompt_blueprint % (system_prompt, user_prompt)\n",
        "\n",
        "    sequences = self.pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=40,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=self.tokenizer.eos_token_id,\n",
        "        max_length=10000,\n",
        "    )\n",
        "\n",
        "    result = sequences[0]['generated_text']\n",
        "\n",
        "    prefix = \"[/INST]\"\n",
        "    return result[result.rfind(prefix)+len(prefix):]\n",
        "\n",
        "  def parse_dataset_items_from_google_data(self, raw_data, input_examples):\n",
        "    input_example = input_examples[0]\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "      You are AUTOMATA BOT: will be given an instruction below, the user will give the input the instruction specifies with the prefix \"AUTOMATA-INPUT\"\n",
        "      {input_example['instruction']}\n",
        "\n",
        "      Your response to the instruction above must start with the prefix tag \"AUTOMATA-RESPONSE:\"\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = []\n",
        "    for i, raw_data_item in enumerate(raw_data):\n",
        "      cleaned_input = self.clean_raw_website_data(raw_data_item, input_example[\"example\"])\n",
        "      print(\"cleaned_input\", cleaned_input)\n",
        "      user_prompt = f\"\"\"\n",
        "        AUTOMATA-INPUT: {cleaned_input}\n",
        "      \"\"\".strip()\n",
        "\n",
        "      print(\"RAW DATA ITEM\", i)\n",
        "      prompt = self.prompt_blueprint % (system_prompt, user_prompt)\n",
        "\n",
        "      sequences = self.pipeline(\n",
        "          prompt ,\n",
        "          do_sample=True,\n",
        "          top_k=40,\n",
        "          num_return_sequences=1,\n",
        "          eos_token_id=self.tokenizer.eos_token_id,\n",
        "          max_length=10000,\n",
        "      )\n",
        "\n",
        "      result = sequences[0]['generated_text']\n",
        "      response_prefix = \"AUTOMATA-RESPONSE:\"\n",
        "\n",
        "      prefix_index = result.rfind(response_prefix)\n",
        "      dataset_item =  f\"AUTOMATA-INPUT: {cleaned_input} {result[prefix_index:]}\"\n",
        "      print(\"\\n\\nDATASET ITEM:\\n\\n\", dataset_item)\n",
        "      dataset.append(dataset_item)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "H-0a7z0QodJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "import numpy as np\n",
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead,TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "# the controller for the base model we're continuously training\n",
        "class TrainingModule:\n",
        "  def __init__(self,\n",
        "      model_name=None,\n",
        "      eval_threshold=.8,\n",
        "      input_example=None,\n",
        "      use_lora=False,\n",
        "      use_qlora=False):\n",
        "    self.dataset_items = []\n",
        "    self.model_name = model_name\n",
        "    self.eval_threshold = eval_threshold\n",
        "    self.input_example = input_example\n",
        "\n",
        "  def set_dataset(self, dataset_items):\n",
        "    self.dataset_items = dataset_items\n",
        "\n",
        "  def load_model(self):\n",
        "    self.model = AutoModelWithLMHead.from_pretrained(self.model_name)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "    self.pipeline = pipeline('text-generation', model='./base-model-output', tokenizer=self.model_name, config={'max_length':1024})\n",
        "\n",
        "  def load_dataset(self,tokenizer):\n",
        "    self.train_dataset = AutoDataset(\n",
        "          self.dataset_items,\n",
        "          self.tokenizer)\n",
        "\n",
        "    self.test_dataset = AutoDataset(\n",
        "          self.dataset_items,\n",
        "          self.tokenizer)\n",
        "\n",
        "    self.data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=self.tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "  def train(self):\n",
        "    self.training_args = TrainingArguments(\n",
        "        output_dir=\"./base-model-output\", #The output directory\n",
        "        overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "        num_train_epochs=3, # number of training epochs\n",
        "        per_device_train_batch_size=32, # batch size for training\n",
        "        per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "        eval_steps=400, # Number of update steps between two evaluations.\n",
        "        save_steps=800, # after # steps model is saved\n",
        "        warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "        prediction_loss_only=True,\n",
        "        )\n",
        "\n",
        "    self.trainer = Trainer(\n",
        "        model=self.model,\n",
        "        args=self.training_args,\n",
        "        data_collator=self.data_collator,\n",
        "        train_dataset=self.train_dataset,\n",
        "        eval_dataset=self.test_dataset,\n",
        "    )\n",
        "\n",
        "    self.trainer.train()\n",
        "\n",
        "\n",
        "  def evaluate(self):\n",
        "    if self.input_example and self.input_example['example_response']:\n",
        "       example_dataset = AutoDataset(\n",
        "          [f\"AUTOMATA-INPUT: {self.input_example['example']}\\nAUTOMATA-RESPONSE: {self.input_example['example_response']}\"],\n",
        "          self.tokenizer)\n",
        "       predictions = self.trainer.predict(example_dataset)\n",
        "    else:\n",
        "      predictions = self.trainer.predict(self.test_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=-1)\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    output = metric.compute(predictions=preds, references=predictions.label_ids)\n",
        "    accuracy = output[\"accuracy\"]\n",
        "\n",
        "    return accuracy > self.eval_threshold\n",
        "\n",
        "\n",
        "  def save_model(self):\n",
        "    self.trainer.save_model()\n"
      ],
      "metadata": {
        "id": "Hx_C1_WUpe1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.parse\n",
        "import time\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "\n",
        "class WebScraper:\n",
        "  def __init__(self, search_engine=\"google\"):\n",
        "    driver_path = '/usr/bin/chromedriver'\n",
        "    chrome_options = Options()\n",
        "\n",
        "\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--remote-debugging-port=9222\")\n",
        "\n",
        "    service = Service(executable_path=driver_path)\n",
        "\n",
        "    self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    self.search_engine = search_engine\n",
        "\n",
        "  def fetch_brave_links(self, brave_query):\n",
        "    wait = WebDriverWait(self.driver, 100)\n",
        "\n",
        "    try:\n",
        "      self.driver.get(f\"https://search.brave.com/search?q={brave_query}&source=desktop\")\n",
        "    except:\n",
        "      return None\n",
        "    # print(self.driver.page_source)\n",
        "\n",
        "    a_selector = \"main #results .snippet > a.h\"\n",
        "\n",
        "    search_result_titles = self.driver.find_elements(By.CSS_SELECTOR, a_selector)\n",
        "    links = []\n",
        "    if len(search_result_titles) == 0:\n",
        "      return None\n",
        "\n",
        "    for link in search_result_titles:\n",
        "      href = link.get_attribute(\"href\")\n",
        "      if href:\n",
        "        links.append(href)\n",
        "\n",
        "    print(\"FOUND_LINKS\", links)\n",
        "    return links\n",
        "\n",
        "  def fetch_google_links(self, google_query):\n",
        "    wait = WebDriverWait(self.driver, 100)\n",
        "    # print(self.driver.text)\n",
        "\n",
        "    try:\n",
        "      self.driver.get(\"https://www.google.com/search?q=\"+google_query)\n",
        "    except:\n",
        "      return None\n",
        "    print(self.driver.page_source)\n",
        "\n",
        "    # print(self.driver.find_element(By.XPATH, \"/html/body\").text)\n",
        "    h3_selector = \"#search [jscontroller][lang][data-ved][jsaction] a h3\"\n",
        "\n",
        "    wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, h3_selector)))\n",
        "\n",
        "    search_result_titles = self.driver.find_elements(By.CSS_SELECTOR, h3_selector)\n",
        "\n",
        "    link_elements = [self.driver.execute_script(\"return arguments[0].closest(\\\"a\\\")\", title_el) for title_el in search_result_titles]\n",
        "\n",
        "    links = []\n",
        "\n",
        "    for link in link_elements:\n",
        "      href = link.get_attribute(\"href\")\n",
        "      if href:\n",
        "        links.append(href)\n",
        "\n",
        "    return links\n",
        "\n",
        "  def fetch_raw_data(self, search_query):\n",
        "    # document.querySelectorAll(\"#search a h3\")\n",
        "    if self.search_engine == \"google\":\n",
        "      links = self.fetch_google_links(search_query)\n",
        "    else:\n",
        "      links = self.fetch_brave_links(search_query)\n",
        "\n",
        "    if not links:\n",
        "      return None\n",
        "    raw_data = []\n",
        "    for i, link in enumerate(links):\n",
        "      if i==3:\n",
        "        break\n",
        "      print(\"Fetching data for\", link)\n",
        "\n",
        "      try:\n",
        "        self.driver.get(link)\n",
        "      except:\n",
        "        print(\"Failed, passing\")\n",
        "        # swallowing error\n",
        "        continue\n",
        "\n",
        "      page_text = self.driver.execute_script(\"return document.body.innerText\")\n",
        "      raw_data.append(page_text)\n",
        "\n",
        "    return raw_data\n",
        "\n"
      ],
      "metadata": {
        "id": "IOMc261zo9nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "Given the full text of a news article, summarize the article in 2-3 sentences\n",
        "\n",
        "Only respond with the summarized article\n",
        "\"\"\"\n",
        "\n",
        "example = \"\"\"\n",
        "Henry Ford (July 30, 1863 – April 7, 1947) was an American industrialist and business magnate. He was the founder of Ford Motor Company, and chief developer of the assembly line technique of mass production. Ford created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century.\n",
        "\n",
        "Ford was born on a farm in Michigan's Springwells Township, leaving home at age 16 to work in Detroit. It was a few years before this time that Ford first experienced automobiles, and throughout the later half of the 1880s, Ford began repairing and later constructing engines, and through the 1890s worked with a division of Edison Electric. He officially founded Ford Motor Company in 1903, after prior failures in business but success in constructing automobiles.\n",
        "\"\"\"\n",
        "\n",
        "example_response = \"\"\"\n",
        "Henry Ford was an American industrialist and business magnate who founded Ford Motor Company, developed the assembly line technique of mass production, and created the first automobile that middle-class Americans could afford, profoundly impacting the landscape of the 20th century\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uOwrzRoRtNNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_raw_data = \"\"\"January 2023\\n\\n(Someone fed my essays into GPT to make something that could answer questions based on them, then asked it where good ideas come from. The answer was ok, but not what I would have said. This is what I would have said.)\\n\\nThe way to get new ideas is to notice anomalies: what seems strange, or missing, or broken? You can see anomalies in everyday life (much of standup comedy is based on this), but the best place to look for them is at the frontiers of knowledge.\\n\\nKnowledge grows fractally. From a distance its edges look smooth, but when you learn enough to get close to one, you'll notice it's full of gaps. These gaps will seem obvious; it will seem inexplicable that no one has tried x or wondered about y. In the best case, exploring such gaps yields whole new fractal buds.\\n\\n\\n\\n\\n\"\"\".strip()"
      ],
      "metadata": {
        "id": "yzXJqjQw2BgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_raw_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPArUmuL2KBZ",
        "outputId": "936f25c2-800b-4114-85bf-ab2365bd0f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "794"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNfiX2yzm7kE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class Automata:\n",
        "  def __init__(self,\n",
        "      reasoning_llm=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "      base_model=\"gpt2\",\n",
        "      input_examples=[{\n",
        "          \"instruction\": instruction,\n",
        "          \"example\": example,\n",
        "          \"example_response\": example_response\n",
        "      }],\n",
        "      max_dataset_size=25,\n",
        "      search_engine=\"google\",\n",
        "      randomize=False,\n",
        "      prefixes={\n",
        "          \"input_prefix\": \"AUTOMATA-INPUT:\",\n",
        "          \"response_prefix\": \"AUTOMATA-RESPONSE:\"\n",
        "      },\n",
        "      eval_threshold=.8):\n",
        "    self.max_dataset_size = max_dataset_size\n",
        "    self.base_model = base_model\n",
        "    self.input_examples = input_examples\n",
        "    self.randomize = randomize\n",
        "    self.prefixes = prefixes\n",
        "    self.eval_threshold = eval_threshold\n",
        "\n",
        "    self.reasoning_llm = ReasoningLLM(model_name=reasoning_llm)\n",
        "    self.training_module = TrainingModule(model_name=base_model,\n",
        "                                          eval_threshold=self.eval_threshold,\n",
        "                                          input_example=self.input_examples[0])\n",
        "    self.web_scraper = WebScraper(search_engine=search_engine)\n",
        "\n",
        "    # state with the current dataset items in the loop\n",
        "    self.current_dataset_items = []\n",
        "\n",
        "  def prune_dataset(self):\n",
        "    # actually prune the dataset eventually\n",
        "    self.current_dataset_items = []\n",
        "\n",
        "  def run_loop(self):\n",
        "    # 0) PRUNE prev dataset based on previous outcome\n",
        "    self.prune_dataset()\n",
        "\n",
        "    # 1) get the dataset\n",
        "    while len(self.current_dataset_items) < self.max_dataset_size:\n",
        "      print(\"dataset size:\", len(self.current_dataset_items))\n",
        "      print(\"Getting a New Google Query\")\n",
        "      google_query = self.reasoning_llm.get_google_query(self.input_examples)\n",
        "      if not google_query:\n",
        "        print(\"Google Query Generation Failed (trying again)\")\n",
        "        continue\n",
        "\n",
        "      print(\"Google Query: \", google_query)\n",
        "      if self.randomize:\n",
        "        google_query = self.reasoning_llm.randomize_query(google_query)\n",
        "\n",
        "      if not google_query:\n",
        "        print(\"Random Query Generation Failed (trying again)\")\n",
        "        continue\n",
        "\n",
        "      print(\"Randomized Query: \", google_query)\n",
        "\n",
        "      print(\"Scraping raw data\")\n",
        "      raw_data = self.web_scraper.fetch_raw_data(google_query)\n",
        "      # raw_data = [text_raw_data]\n",
        "      if not raw_data:\n",
        "        print(\"Fetching google failed\")\n",
        "        continue\n",
        "\n",
        "      print(\"Parsing dataset items from raw data\")\n",
        "      dataset_items = self.reasoning_llm.parse_dataset_items_from_google_data(raw_data, self.input_examples)\n",
        "      self.current_dataset_items += dataset_items\n",
        "\n",
        "    # 2) train the base model\n",
        "    self.training_module.set_dataset(self.current_dataset_items)\n",
        "    self.training_module.load_model()\n",
        "    self.training_module.load_dataset()\n",
        "\n",
        "    self.training_module.train()\n",
        "\n",
        "    eval_pass = self.training_module.evaluate()\n",
        "\n",
        "    # 3) decision point: evaluate and see if it's good enough\n",
        "    if eval_pass:\n",
        "      print(\"Automation finished, saving your dataset and saving your final model\")\n",
        "      # save model\n",
        "      self.training_module.save_model()\n",
        "      # save dataset\n",
        "      with open(\"./automata-dataset.json\", \"w+\") as f:\n",
        "        json.dump(self.current_dataset_items, f)\n",
        "\n",
        "      return True\n",
        "    else:\n",
        "    # 4) restart loop if dataset is does not pass evaluation\n",
        "      return self.run_loop()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "automata = Automata(search_engine=\"brave\", randomize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "e96db59a99b64fba83a416a42ffc34af",
            "5414c837886c4374a8dc6edd8fd50554",
            "0b88e31c55f24feca59303cab68a95aa",
            "760c682ac04e4dadb58579f73765ee80",
            "f300ef0513134f8f8a6ce21e38bd3de7",
            "a7d4ce8b45664f95bba494ccb0c7aaae",
            "0cdf4f7c29744eef95f1762cb4965549",
            "28ab8dca5b4b4ac5b638897bde93f801",
            "f472e18f02fc4b51bea225246c696470",
            "c43aaf9cb25648fc9ce97ef371c60308",
            "41ea07f7290b4c808e2affbcce1f21d6"
          ]
        },
        "id": "q3gbeQVQoceH",
        "outputId": "a25bee7c-a8b2-481b-c2e5-16e9f9117473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e96db59a99b64fba83a416a42ffc34af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "automata.run_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CqR3g6QQ-pio",
        "outputId": "9d99a1ab-9a2c-4d59-baae-863ca2a41fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing dataset items from raw data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned_input \n",
            "  \n",
            "    CLEANED WEBSITE DATA: \n",
            "Henry Ford was an American industrialist and business magnate. He was the founder of Ford Motor Company and chief developer of the assembly line technique of mass production. He created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century. Ford was born on a farm in Michigan's Springwells Township, leaving home at age 16 to work in Detroit. It was a few years before this time that Ford first experienced automobiles.\n",
            "\n",
            "Through the 1880s, Ford began repairing and later constructing engines and, through the 1890s, worked with a division of Edison Electric. He officially founded Ford Motor Company in 1903. Ford's approach to business focused on efficiency, and he played a crucial role in the industrial expansion of the United States during the early 20th century.\n",
            "\n",
            "The best place to look for anomalies in everyday life is at the frontiers of knowledge, where there are gaps that seem inexplicable. Exploring such gaps can yield new ideas and new fractal buds. The growth of knowledge is fractal, with smooth edges from a distance but gaps that become more obvious and inexplicable as you get closer. At the best of these places, new ideas can lead to significant fractal buds.\n",
            "RAW DATA ITEM 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "DATASET ITEM:\n",
            "\n",
            " AUTOMATA-INPUT: \n",
            "  \n",
            "    CLEANED WEBSITE DATA: \n",
            "Henry Ford was an American industrialist and business magnate. He was the founder of Ford Motor Company and chief developer of the assembly line technique of mass production. He created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century. Ford was born on a farm in Michigan's Springwells Township, leaving home at age 16 to work in Detroit. It was a few years before this time that Ford first experienced automobiles.\n",
            "\n",
            "Through the 1880s, Ford began repairing and later constructing engines and, through the 1890s, worked with a division of Edison Electric. He officially founded Ford Motor Company in 1903. Ford's approach to business focused on efficiency, and he played a crucial role in the industrial expansion of the United States during the early 20th century.\n",
            "\n",
            "The best place to look for anomalies in everyday life is at the frontiers of knowledge, where there are gaps that seem inexplicable. Exploring such gaps can yield new ideas and new fractal buds. The growth of knowledge is fractal, with smooth edges from a distance but gaps that become more obvious and inexplicable as you get closer. At the best of these places, new ideas can lead to significant fractal buds. AUTOMATA-RESPONSE: Henry Ford was an American industrialist who founded the Ford Motor Company and created the assembly line technique of mass production. He made automobiles affordable for the middle class, impacting the 20th century. He discovered and explored anomalies at the frontiers of knowledge, leading to new ideas and significant fractal buds.\n",
            "Parsing dataset items from raw data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned_input \n",
            "  \n",
            "    EXAMPLE-INFO: \n",
            "Henry Ford (July 30, 1863 – April 7, 1947) was an American industrialist and business magnate. He was the founder of Ford Motor Company, and chief developer of the assembly line technique of mass production. Ford created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century.\n",
            "\n",
            "raw website data: January 2023\n",
            "RAW DATA ITEM 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "DATASET ITEM:\n",
            "\n",
            " AUTOMATA-INPUT: \n",
            "  \n",
            "    EXAMPLE-INFO: \n",
            "Henry Ford (July 30, 1863 – April 7, 1947) was an American industrialist and business magnate. He was the founder of Ford Motor Company, and chief developer of the assembly line technique of mass production. Ford created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century.\n",
            "\n",
            "raw website data: January 2023 AUTOMATA-RESPONSE:\n",
            "\n",
            "Henry Ford was an American industrialist who founded the Ford Motor Company and developed the assembly line technique of mass production. His innovations made automobiles accessible to the middle class and greatly impacted the 20th century landscape. Raw website data from January 2023.\n",
            "Parsing dataset items from raw data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned_input \n",
            "  \n",
            "Thank you for providing the example and raw website data.\n",
            "\n",
            "I have analyzed the example and determined that the information is an article. It presents information about Henry Ford, including his background, achievements, and impact on the world.\n",
            "\n",
            "Based on the information provided in the example, I have identified the following relevant information types and have extracted them from the raw website data:\n",
            "\n",
            "* Biography: Details about Henry Ford's life, including his birthdate, place, and experiences.\n",
            "* Technology: Information about Ford's contributions to the assembly line technique and the automobile industry.\n",
            "* Achievements: Details about the impact of Ford's work on society and the landscape of the 20th century.\n",
            "\n",
            "I have also removed all headers, footers, and general website information that is not considered content.\n",
            "\n",
            "cleaned\\_website\\_data:\n",
            "\n",
            "Henry Ford (July 30, 1863 – April 7, 1947) was an American industrialist and business magnate. He was the founder of Ford Motor Company, and chief developer of the assembly line technique of mass production. Ford created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century.\n",
            "\n",
            "Ford was born on a farm in Michigan's Springwells Township, leaving home at age 16 to work in Detroit. It was a few years before this time that Ford first experienced automobiles, and throughout the later half of the 1880s, Ford began repairing and later constructing engines, and through the 1890s worked with a division of Edison Electric. He officially founded Ford Motor Company in 1903, after prior failures in business but success in constructing automobiles.\n",
            "\n",
            "Ford's conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century. The first automobile that middle-class Americans could afford was the Model T, which was introduced in 1908. At the time, the average American family could not afford a car, but the Model T was priced at $500, well within the budget of most families.\n",
            "\n",
            "The Model T also had a major impact on the way the automobile was built. Prior to the Model T, cars were built one at a time by skilled craftsmen. However, the Model T was produced using an assembly line, which significantly increased the speed and efficiency of the manufacturing process.\n",
            "\n",
            "The assembly line technique of mass production revolutionized the automobile industry and led to a significant increase in the availability and affordability of cars. The Model T had a major impact on transportation and helped to shape the 20th century.\n",
            "\n",
            "Henry Ford's impact on industry and society cannot be overstated. He transformed the automobile from a luxury item into a basic necessity, making it affordable for the average family. He also revolutionized the way cars were built, making it possible to produce them in large quantities at a lower cost.\n",
            "\n",
            "Ford's influence went beyond the automobile industry, as his innovations in mass production had a profound impact on other industries as well. The assembly line technique was adapted for use in other factories and helped to increase efficiency and productivity in a wide range of industries.\n",
            "\n",
            "In conclusion, Henry Ford was a pioneer in the automobile industry and his innovations in mass production had a major impact on society and the landscape of the 20th century. Ford's conversion of the automobile from an expensive luxury into an accessible conveyance had a major impact on transportation and helped to shape the 20th century. Ford's ability to identify problems and exploit gaps in knowledge allowed him to pioneer new approaches to manufacturing and transform industries.\n",
            "RAW DATA ITEM 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "DATASET ITEM:\n",
            "\n",
            " AUTOMATA-INPUT: \n",
            "  \n",
            "Thank you for providing the example and raw website data.\n",
            "\n",
            "I have analyzed the example and determined that the information is an article. It presents information about Henry Ford, including his background, achievements, and impact on the world.\n",
            "\n",
            "Based on the information provided in the example, I have identified the following relevant information types and have extracted them from the raw website data:\n",
            "\n",
            "* Biography: Details about Henry Ford's life, including his birthdate, place, and experiences.\n",
            "* Technology: Information about Ford's contributions to the assembly line technique and the automobile industry.\n",
            "* Achievements: Details about the impact of Ford's work on society and the landscape of the 20th century.\n",
            "\n",
            "I have also removed all headers, footers, and general website information that is not considered content.\n",
            "\n",
            "cleaned\\_website\\_data:\n",
            "\n",
            "Henry Ford (July 30, 1863 – April 7, 1947) was an American industrialist and business magnate. He was the founder of Ford Motor Company, and chief developer of the assembly line technique of mass production. Ford created the first automobile that middle-class Americans could afford, and his conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century.\n",
            "\n",
            "Ford was born on a farm in Michigan's Springwells Township, leaving home at age 16 to work in Detroit. It was a few years before this time that Ford first experienced automobiles, and throughout the later half of the 1880s, Ford began repairing and later constructing engines, and through the 1890s worked with a division of Edison Electric. He officially founded Ford Motor Company in 1903, after prior failures in business but success in constructing automobiles.\n",
            "\n",
            "Ford's conversion of the automobile from an expensive luxury into an accessible conveyance profoundly impacted the landscape of the 20th century. The first automobile that middle-class Americans could afford was the Model T, which was introduced in 1908. At the time, the average American family could not afford a car, but the Model T was priced at $500, well within the budget of most families.\n",
            "\n",
            "The Model T also had a major impact on the way the automobile was built. Prior to the Model T, cars were built one at a time by skilled craftsmen. However, the Model T was produced using an assembly line, which significantly increased the speed and efficiency of the manufacturing process.\n",
            "\n",
            "The assembly line technique of mass production revolutionized the automobile industry and led to a significant increase in the availability and affordability of cars. The Model T had a major impact on transportation and helped to shape the 20th century.\n",
            "\n",
            "Henry Ford's impact on industry and society cannot be overstated. He transformed the automobile from a luxury item into a basic necessity, making it affordable for the average family. He also revolutionized the way cars were built, making it possible to produce them in large quantities at a lower cost.\n",
            "\n",
            "Ford's influence went beyond the automobile industry, as his innovations in mass production had a profound impact on other industries as well. The assembly line technique was adapted for use in other factories and helped to increase efficiency and productivity in a wide range of industries.\n",
            "\n",
            "In conclusion, Henry Ford was a pioneer in the automobile industry and his innovations in mass production had a major impact on society and the landscape of the 20th century. Ford's conversion of the automobile from an expensive luxury into an accessible conveyance had a major impact on transportation and helped to shape the 20th century. Ford's ability to identify problems and exploit gaps in knowledge allowed him to pioneer new approaches to manufacturing and transform industries. AUTOMATA-RESPONSE: \n",
            "\n",
            "Based on the information provided, Henry Ford was an American industrialist and business magnate who revolutionized the automobile industry through his innovations in mass production. His invention of the assembly line technique made car manufacturing more efficient and affordable, transforming the automobile from a luxury item into a basic necessity. Ford's impact went beyond the automobile industry, as his innovations in mass production had a profound impact on other industries as well. The Model T, introduced by Ford in 1908, had a major impact on transportation and helped to shape the 20th century.\n",
            "Parsing dataset items from raw data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned_input \n",
            "  \n",
            "    EXAMPLE-INFO:\n",
            "    The way to get new ideas is to notice anomalies: what seems strange, or missing, or broken? You can see anomalies in everyday life (much of standup comedy is based on this), but the best place to look for them is at the frontiers of knowledge.\n",
            "\n",
            "    RAW WEBSITE DATA: January 2023\n",
            "    \n",
            "      This is not a structured text data, it seems like a text from a speaker or an essay. It does not match the example info provided, which was a paragraph of text describing a person. Therefore, there is no structured information to extract from this data.\n",
            "RAW DATA ITEM 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "DATASET ITEM:\n",
            "\n",
            " AUTOMATA-INPUT: \n",
            "  \n",
            "    EXAMPLE-INFO:\n",
            "    The way to get new ideas is to notice anomalies: what seems strange, or missing, or broken? You can see anomalies in everyday life (much of standup comedy is based on this), but the best place to look for them is at the frontiers of knowledge.\n",
            "\n",
            "    RAW WEBSITE DATA: January 2023\n",
            "    \n",
            "      This is not a structured text data, it seems like a text from a speaker or an essay. It does not match the example info provided, which was a paragraph of text describing a person. Therefore, there is no structured information to extract from this data. AUTOMATA-RESPONSE:\" \n",
            "    \n",
            "  <</SYS>>\n",
            "  AUTOMATA-INPUT: \n",
            "  \n",
            "    EXAMPLE-INFO:\n",
            "    The way to get new ideas is to notice anomalies: what seems strange, or missing, or broken? You can see anomalies in everyday life (much of standup comedy is based on this), but the best place to look for them is at the frontiers of knowledge.\n",
            "\n",
            "    RAW WEBSITE DATA: January 2023\n",
            "    \n",
            "      This is not a structured text data, it seems like a text from a speaker or an essay. It does not match the example info provided, which was a paragraph of text describing a person. Therefore, there is no structured information to extract from this data.\n",
            "  [/INST]\n",
            "  \n",
            "  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "  \n",
            "  <automatabot>\n",
            "    \n",
            "      <instruction>\n",
            "        \n",
            "          <text>Summary an example text.</text>\n",
            "        \n",
            "      </instruction>\n",
            "      <output>\n",
            "        \n",
            "          <response>\n",
            "            <text>This is not a structured text data, it seems like a text from a speaker or an essay. It does not match the example info provided, which was a paragraph of text describing a person. Therefore, there is no structured information to extract from this data. </text>\n",
            "          </response>\n",
            "        \n",
            "      </output>\n",
            "  \n",
            "  \n",
            "  </automatabot>\n",
            "Parsing dataset items from raw data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e98eba86f17a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautomata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-b3c834d9e3e4>\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parsing dataset items from raw data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m       \u001b[0mdataset_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreasoning_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_dataset_items_from_google_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_dataset_items\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdataset_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d7f36c543ea9>\u001b[0m in \u001b[0;36mparse_dataset_items_from_google_data\u001b[0;34m(self, raw_data, input_examples)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0mcleaned_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_raw_website_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"example\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cleaned_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m       user_prompt = f\"\"\"\n",
            "\u001b[0;32m<ipython-input-3-d7f36c543ea9>\u001b[0m in \u001b[0;36mclean_raw_website_data\u001b[0;34m(self, raw_data, example)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_blueprint\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msystem_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     sequences = self.pipeline(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             )\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 )\n\u001b[1;32m    620\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    622\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself_attn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}